{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# nn contains all of PyTorch's building blocks for neural networks\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from visdom import Visdom\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "learning_rate = 0.01\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_db = datasets.MNIST('../data', train=True, download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(), \n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ]))\n",
    "train_loader = torch.utils.data.DataLoader(train_db,batch_size=batch_size, shuffle=True)\n",
    "test_db = datasets.MNIST('../data', train=False, download=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(), \n",
    "                    #    transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ]))\n",
    "test_loader = torch.utils.data.DataLoader(test_db,batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_db, val_db = torch.utils.data.random_split(train_db, [50000, 10000])\n",
    "train_loader = torch.utils.data.DataLoader(train_db, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_db, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 200),\n",
    "            # torch.nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(200, 200),\n",
    "            # torch.nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(200, 10),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "viz = Visdom()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = MLP().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# 监听最小值，当验证集的loss不再下降时，减小学习率，patience=5表示连续5次不下降就减小学习率\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "# 对 epoch 记录，每10个epoch减小学习率，gamma=0.1表示变为0.1倍\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "criteon = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "viz.line([0.], [0.], win='train_loss', opts=dict(title='train loss'))\n",
    "viz.line([[0.0, 0.0]], [0.], win='test', opts=dict(title='test loss&acc.',\n",
    "                                                   legend=['loss', 'acc.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.299768\n",
      "Train Epoch: 0 [20000/50000 (40%)]\tLoss: 0.431636\n",
      "Train Epoch: 0 [40000/50000 (80%)]\tLoss: 0.213986\n",
      "\\Val set: Average loss: 0.0013, Accuracy: 9220.0/10000 (92.20%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.221547\n",
      "Train Epoch: 1 [20000/50000 (40%)]\tLoss: 0.250048\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 0.225614\n",
      "\\Val set: Average loss: 0.0009, Accuracy: 9458.0/10000 (94.58%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.157654\n",
      "Train Epoch: 2 [20000/50000 (40%)]\tLoss: 0.116677\n",
      "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 0.114392\n",
      "\\Val set: Average loss: 0.0008, Accuracy: 9536.0/10000 (95.36%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.108623\n",
      "Train Epoch: 3 [20000/50000 (40%)]\tLoss: 0.140196\n",
      "Train Epoch: 3 [40000/50000 (80%)]\tLoss: 0.111087\n",
      "\\Val set: Average loss: 0.0006, Accuracy: 9627.0/10000 (96.27%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.112550\n",
      "Train Epoch: 4 [20000/50000 (40%)]\tLoss: 0.079489\n",
      "Train Epoch: 4 [40000/50000 (80%)]\tLoss: 0.124655\n",
      "\\Val set: Average loss: 0.0005, Accuracy: 9665.0/10000 (96.65%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.072015\n",
      "Train Epoch: 5 [20000/50000 (40%)]\tLoss: 0.052807\n",
      "Train Epoch: 5 [40000/50000 (80%)]\tLoss: 0.035061\n",
      "\\Val set: Average loss: 0.0005, Accuracy: 9709.0/10000 (97.09%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.101672\n",
      "Train Epoch: 6 [20000/50000 (40%)]\tLoss: 0.072394\n",
      "Train Epoch: 6 [40000/50000 (80%)]\tLoss: 0.124134\n",
      "\\Val set: Average loss: 0.0005, Accuracy: 9711.0/10000 (97.11%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.023057\n",
      "Train Epoch: 7 [20000/50000 (40%)]\tLoss: 0.084775\n",
      "Train Epoch: 7 [40000/50000 (80%)]\tLoss: 0.086364\n",
      "\\Val set: Average loss: 0.0004, Accuracy: 9734.0/10000 (97.34%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.027158\n",
      "Train Epoch: 8 [20000/50000 (40%)]\tLoss: 0.032154\n",
      "Train Epoch: 8 [40000/50000 (80%)]\tLoss: 0.073976\n",
      "\\Val set: Average loss: 0.0004, Accuracy: 9728.0/10000 (97.28%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.034666\n",
      "Train Epoch: 9 [20000/50000 (40%)]\tLoss: 0.060701\n",
      "Train Epoch: 9 [40000/50000 (80%)]\tLoss: 0.017936\n",
      "\\Val set: Average loss: 0.0004, Accuracy: 9748.0/10000 (97.48%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "best_accuracy = 0\n",
    "best_model_path = 'models/'\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(-1, 28*28).to(device)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        logits = model(data)\n",
    "        loss = criteon(logits, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "        viz.line([loss.item()], [global_step], win='train_loss', update='append')\n",
    "\n",
    "        if batch_idx % 100 == 0: \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in val_loader:\n",
    "        data = data.view(-1, 28*28)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        logits = model(data)\n",
    "        test_loss += criteon(logits, target).item()\n",
    "        # 监听最小值，当验证集的loss不再下降时，减小学习率\n",
    "        # scheduler.step(test_loss)\n",
    "        pred = logits.argmax(dim=1)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target).float().sum().item()\n",
    "\n",
    "    viz.line([[test_loss, correct / len(val_loader.dataset)]], [global_step], win='val', update='append')\n",
    "    viz.images(data.view(-1, 1, 28, 28), win='val_x',opts={'title':'val_x'})\n",
    "    viz.text(str(pred.detach().cpu().numpy()), win='val_pred', opts={'title':'val_pred'})\n",
    "    \n",
    "    test_loss /= len(val_loader.dataset)\n",
    "    accuracy = 100. * correct / len(val_loader.dataset)\n",
    "    print('\\Val set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(val_loader.dataset),\n",
    "        accuracy))\n",
    "    \n",
    "    # if accuracy > best_accuracy:\n",
    "    #     best_accuracy = accuracy\n",
    "    #     torch.save(model.state_dict(), best_model_path + f'{epoch}_{accuracy:.4f}_best_model.pth')\n",
    "    #     print(f\"Best model saved with accuracy: {best_accuracy:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Val set: Average loss: 0.0025, Accuracy: 9677.0/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data = data.view(-1, 28*28)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    logits = model(data)\n",
    "    test_loss += criteon(logits, target).item()\n",
    "    \n",
    "    pred = logits.argmax(dim=1)  # get the index of the max log-probability\n",
    "    correct += pred.eq(target).float().sum().item()\n",
    "\n",
    "viz.line([[test_loss, correct / len(test_loader.dataset)]], [global_step], win='test', update='append')\n",
    "viz.images(data.view(-1, 1, 28, 28), win='x')\n",
    "viz.text(str(pred.detach().cpu().numpy()), win='test_pred', opts={'title':'test_pred'})\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\Val set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用记录\n",
    "\n",
    "没有加dropout\n",
    "result:\n",
    "\\Val set: Average loss: 0.0004, Accuracy: 9743.0/10000 (97%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
